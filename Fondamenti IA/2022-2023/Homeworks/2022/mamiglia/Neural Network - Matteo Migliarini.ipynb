{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Neural Network - Matteo Migliarini","provenance":[{"file_id":"1RumCaRMsGj019RAGd0YJEszWyVvX64QU","timestamp":1652824396907}],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["\n","# Neural Network Code\n","---\n"],"metadata":{"id":"frHuxJS5Agsg"}},{"cell_type":"markdown","source":["## Layer\n","The Neural Network is structured in Layers. Each layer contains the matrix $W$ and an activation function $\\sigma$. By default $\\sigma(x) = LeakyReLU(x)$. The weight matrix $W$ is initialized using the _He-et-el_ initialization and to prevent gradient exploding I use the gradient clipping method.\n"],"metadata":{"id":"txLywfp5Btep"}},{"cell_type":"code","execution_count":1,"metadata":{"id":"ipjmtzVCheJt","executionInfo":{"status":"ok","timestamp":1652982061725,"user_tz":-120,"elapsed":6,"user":{"displayName":"matteo migliarini","userId":"13409704682108753227"}}},"outputs":[],"source":["class Layer:\n","    ReLU = lambda x :  np.maximum(0, x)\n","    DE_ReLU = lambda x :  x > 0\n","    LEAKY_ReLU = lambda x : np.where(x > 0, x, x * 0.01)\n","    DE_LEAKY_ReLU = lambda x :  np.where(x > 0, 1, 0.01)\n","    LINEAR = lambda x: x\n","    DE_LINEAR = lambda x: 1\n","    SIGMOID = lambda x: 1/(1+np.exp(x))\n","    DE_SIGMOID = lambda x: Layer.SIGMOID(x)*(1-Layer.SIGMOID(x))\n","    DELTA_THRESHOLD = 5\n","    \n","    def __init__(self, input_size, output_size, ALPHA=.1):\n","        #He-et-al Initialization.\n","        self.W = np.random.randn(input_size, output_size) * np.sqrt(2 / output_size) \n","        self.bias = np.random.randn(1, output_size) * np.sqrt(2 / output_size)\n","        # default activation function is Leaky ReLU\n","        self.SIGMA =  Layer.LEAKY_ReLU \n","        self.DE_SIGMA = Layer.DE_LEAKY_ReLU\n","        \n","        self.input = None\n","        self.z = None\n","        self.ALPHA = ALPHA\n","        \n","    def forward(self, a):\n","        self.input = a # saves input for backprop later\n","        self.z = a.dot(self.W) + self.bias\n","        return self.SIGMA(self.z)\n","    \n","    def backprop(self, output_error):\n","        output_error = self.DE_SIGMA(self.z) * output_error\n","        delta = output_error @ self.W.T # delta for the next level\n","        delta = Layer.gradient_clipping(delta) # to prevent gradient exploding\n","\n","        dW = self.input.T.dot(output_error)\n","        dB = output_error\n","\n","        self.W -= self.ALPHA * dW\n","        self.bias -= self.ALPHA * dB\n","        return delta\n","    \n","    @staticmethod\n","    def gradient_clipping( delta ):\n","        return np.where(abs(delta)<Layer.DELTA_THRESHOLD, delta, abs(delta)/delta*Layer.DELTA_THRESHOLD)\n","    \n","    def __str__(self):\n","        return f\"Layer {self.W.shape[0]}x{self.W.shape[1]}.\"\n"]},{"cell_type":"markdown","source":["## Neural Network class\n","Since this excercise is about regression, the last layer uses $linear$ activation rather then the default $LeakyReLU$.\n","The user may use the initialization parameters `hidden_layers, epochs, batch_size, learning_rate` in order to adjust the hyperparameters of the algorithm. \n"],"metadata":{"id":"knAfBlQEhlhC"}},{"cell_type":"code","source":["class NeuralNetwork:    \n","    def __init__(self, input_size, output_size=1, hidden_layers=[4,2], epochs=1000, batch_size=20, learning_rate=0.1):\n","        l = [input_size] + hidden_layers + [output_size]\n","        self.layers = [Layer(l[i], l[i+1], ALPHA=learning_rate) for i in range(len(l)-1)]\n","        #last 2 layers have linear activation (regression)\n","        self.layers[-1].SIGMA = Layer.LINEAR \n","        self.layers[-1].DE_SIGMA = Layer.DE_LINEAR\n","        self.EPOCH = epochs\n","        self.BATCH = batch_size\n","        \n","        \n","    def fit(self, training_set, target_set):\n","        err = 1\n","        epoch=0\n","        while (epoch<self.EPOCH and err>0.001):\n","            batch = np.random.choice(len(training_set), size=self.BATCH, replace=False)\n","            X = training_set[batch]\n","            Y = target_set[batch]\n","            err=0\n","            for (x,y) in zip(X,Y):\n","                x = x.reshape(1,x.size)\n","                h = self.predict(x)\n","                # print(x, y, h)\n","                err += (y-h)**2 \n","            \n","            \n","                delta = (h-y)\n","                for layer in self.layers[::-1]:\n","                    delta = layer.backprop(delta)\n","                    \n","            epoch+=1\n","            if (epoch+1)%(self.EPOCH/10) == 0:\n","                err /= self.BATCH\n","                print(f\"epoch {epoch+1} error rate is {err}\")\n","                # for l in self.layers:\n","                #     print(l.W)\n","                \n","        print(f\"Error is {err} at epoch {epoch+1}\")\n","    \n","    def predict(self, x):\n","        output = x\n","        for layer in self.layers:\n","            output = layer.forward(output)\n","            \n","        return output \n","    \n","    def __str__(self):\n","        return \"\\n\".join([f\"{str(L)}\\nW is:\\n{L.W}\\nB is: {L.bias} \" for L in self.layers])\n"],"metadata":{"id":"rP40HriJhsN0","executionInfo":{"status":"ok","timestamp":1652982062058,"user_tz":-120,"elapsed":5,"user":{"displayName":"matteo migliarini","userId":"13409704682108753227"}}},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":["## Utility\n","Some utility functions to evaluate the Neural Network and prepare the dataset"],"metadata":{"id":"5Vo69yvUh69x"}},{"cell_type":"code","source":["import numpy as np\n","def random_permutation(size, *arrays):\n","    seed = np.random.permutation(size)\n","    ret = list()\n","\n","    for a in arrays:\n","        ret.append(a[seed])\n","    \n","    return tuple(ret)\n","\n","def partition_dataset(percentage, x, y):\n","    size = len(x)\n","    training_size = round(percentage*size)\n","    x,y = random_permutation(size, x,y)\n","    \n","    return x[:training_size], y[:training_size], x[training_size:], y[training_size:]\n","\n","def evaluate(nn, data, target):\n","    count = 0\n","    error = 0\n","    for (x,y) in zip(data, target):\n","        p = nn.predict(x)[0][0]\n","        # print(x, y, p)\n","        error += abs(y-p)/data.shape[0]\n","        if (round(p) == y):\n","            count += 1\n","    print(f\"Accuracy: {round(count/len(data)*100, 2)}%, error rate = {error}\")\n","    return count/len(data)"],"metadata":{"id":"IMcDLcaEh7ZW","executionInfo":{"status":"ok","timestamp":1652982065532,"user_tz":-120,"elapsed":4,"user":{"displayName":"matteo migliarini","userId":"13409704682108753227"}}},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":["# Model Evaluation\n","---"],"metadata":{"id":"qmFIvx6TDP8C"}},{"cell_type":"markdown","source":["## XOR\n","Accuracy is decent, even though it heavily depends on the initial values of $W_i$"],"metadata":{"id":"z0qmV6IL_t0X"}},{"cell_type":"code","source":["XOR_train = np.array([[0,0], [0,1], [1,0], [1,1]])\n","XOR_target = np.array([[0], [1], [1], [0]])\n","        \n","nn_XOR = NeuralNetwork(XOR_train[0].shape[0], hidden_layers=[16,4], batch_size=3, epochs=1000, learning_rate=0.01)\n","nn_XOR.fit(XOR_train, XOR_target)\n","acc=evaluate(nn_XOR, XOR_train, XOR_target)\n"],"metadata":{"id":"wKrlJ2sliIy3","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1652982068130,"user_tz":-120,"elapsed":455,"user":{"displayName":"matteo migliarini","userId":"13409704682108753227"}},"outputId":"86216319-b5e4-4dec-c4eb-90818c52641c"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["epoch 100 error rate is [[0.21001296]]\n","epoch 200 error rate is [[0.18193575]]\n","epoch 300 error rate is [[0.14103276]]\n","epoch 400 error rate is [[0.09420524]]\n","epoch 500 error rate is [[0.04609542]]\n","epoch 600 error rate is [[0.01023063]]\n","epoch 700 error rate is [[0.00434453]]\n","Error is [[0.00093082]] at epoch 733\n","Accuracy: 100.0%, error rate = [0.03482724]\n"]}]},{"cell_type":"markdown","source":["## Iris\n","Due to a much larger dataset, Iris results are much more cleaner, consistent and less dependent on the initial $W_i$"],"metadata":{"id":"ozX_QKkNibar"}},{"cell_type":"code","source":["from sklearn.datasets import load_iris\n","dataset = load_iris()\n","TRAINING_RATIO = .4\n","\n","data_train, target_train, data_validate, target_validate = partition_dataset(TRAINING_RATIO, dataset.data, dataset.target)\n","\n","\n","nn = NeuralNetwork(data_train[0].shape[0], hidden_layers=[7,10,3], learning_rate=0.01)\n","nn.fit(data_train, target_train)\n","acc = evaluate(nn, data_validate, target_validate)\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BBXBs9tFiOEc","executionInfo":{"status":"ok","timestamp":1652982075709,"user_tz":-120,"elapsed":6048,"user":{"displayName":"matteo migliarini","userId":"13409704682108753227"}},"outputId":"c3d490df-87c2-400f-c771-be70b789e080"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["epoch 100 error rate is [[0.23550599]]\n","epoch 200 error rate is [[0.05275016]]\n","epoch 300 error rate is [[0.08158702]]\n","epoch 400 error rate is [[0.05524922]]\n","epoch 500 error rate is [[2.12595406]]\n","epoch 600 error rate is [[0.05333887]]\n","epoch 700 error rate is [[0.06819046]]\n","epoch 800 error rate is [[0.0213874]]\n","epoch 900 error rate is [[0.06546229]]\n","epoch 1000 error rate is [[0.045872]]\n","Error is [[1.41223645]] at epoch 1001\n","Accuracy: 98.89%, error rate = 0.1594247612529447\n"]}]}]}